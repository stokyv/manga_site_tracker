{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3258380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json \n",
    "import os\n",
    "import os.path\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import doctest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20bf28df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circle(href):\n",
    "    return href and re.compile(\"circle\").search(href)\n",
    "def item(href):\n",
    "    return href and re.compile(\"item\").search(href)\n",
    "\n",
    "def price(string):\n",
    "    return string and re.compile(\"円\").search(string)\n",
    "\n",
    "def get_price(tag_string):\n",
    "    amount = tag_string.replace('円','').strip()\n",
    "    amount = amount.replace(',','')   \n",
    "    return int(float(amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "394def80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_price_format(string):\n",
    "    try:\n",
    "        return get_price(string)\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66fbb555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(url):    \n",
    "    resp = requests.get(url)\n",
    "    print(resp.status_code)\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def find_main_product_div(soup):\n",
    "    #find the main_div\n",
    "    main_div = soup.find_all('div', class_='product-list')[0] #only one match\n",
    "    return main_div\n",
    "\n",
    "def get_product_div(main_div):\n",
    "    \"\"\"return a list of divs of products from the main div  \n",
    "    \"\"\"\n",
    "    #store all product divs\n",
    "    item_div_list = []\n",
    "    for child in main_div.children:\n",
    "        if child == '\\n':\n",
    "            continue\n",
    "        else:\n",
    "            item_div_list.append(child)\n",
    "    return item_div_list\n",
    "\n",
    "def extract_info(item_tag):\n",
    "    \"\"\"from an item tag, extract and return a dictionary of relevant info of the item\"\"\"\n",
    "    item_info = {}\n",
    "    \n",
    "    #get item url and name\n",
    "    results = item_tag.find_all(href=item)\n",
    "    result = results[1] # use the second tag b/c easier to retrieve string\n",
    "    item_info['item_name'] = result.text.strip()\n",
    "    item_info['item_url'] = base_url + result['href']\n",
    "    \n",
    "\n",
    "    #get circle name and url\n",
    "    results = item_tag.find_all(href=circle)\n",
    "    if not results:\n",
    "        item_info['circle_name'] = None\n",
    "        item_info['circle_url'] = None\n",
    "    else:\n",
    "        result = results[0]\n",
    "        item_info['circle_name'] = result.text.strip()\n",
    "        item_info['circle_url'] = base_url + result['href']    \n",
    "    \n",
    "    #get price\n",
    "    #temporary solutions -> need to extract exact tag string!\n",
    "    results = item_tag.find_all(string=price)\n",
    "    amount = 0\n",
    "    for string in results:\n",
    "        if check_price_format(string):\n",
    "            amount = get_price(string)\n",
    "#     amount = get_price(price_string)\n",
    "    item_info['item_price'] = amount\n",
    "    \n",
    "    return item_info\n",
    "\n",
    "def extract_all_info(item_div_list):\n",
    "    \"\"\"return a dictionary of best selling items from 1 to 100\"\"\"\n",
    "    best_sellers = {}\n",
    "    i = 1\n",
    "    for item_tag in item_div_list:\n",
    "        item_info = extract_info(item_tag)\n",
    "        best_sellers[i] = item_info\n",
    "        i += 1\n",
    "    return best_sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4523ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format filename\n",
    "#examples of date tex\n",
    "txt = '[集計期間：2021年07月03日 00時~ 2021年07月09日 24時]'\n",
    "\n",
    "def extract_date_info(soup):\n",
    "    \"\"\"return a list of two tuples of (year, month, day) of the best seller url\"\"\"\n",
    "    date_text = soup.find_all('p', class_='heading01_leadtext')[0].text\n",
    "    p = re.compile(r'(\\d+)年(\\d+)月(\\d+)日')\n",
    "    groups = []\n",
    "    for group in p.findall(date_text):\n",
    "        groups.append(group)\n",
    "    return groups\n",
    "\n",
    "# issue: need separate base_name i.e date as key in dict -> a new func to format base-name\n",
    "\n",
    "def format_basename(date_groups, format_,):\n",
    "    \"\"\"\n",
    "    Return the formatted filename for either daily or weekly or monthly rankings \n",
    "    \"\"\"\n",
    "    if format_ == 'd':\n",
    "        base_name = ''.join(date_groups[0])\n",
    "    elif format_ == 'w':\n",
    "        base_name = '_'.join([''.join(group) for group in date_groups])\n",
    "    elif format_ == 'm':\n",
    "        base_name = '_'.join(date_groups[0][:2])\n",
    "\n",
    "    return base_name\n",
    "\n",
    "def format_filename(base_name, ext='json'):\n",
    "    return base_name + '.' + ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4bfb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tora_doujinshi_daily = 'https://ec.toranoana.jp/tora_r/ec/cot/ranking/daily/all'\n",
    "tora_book_daily = 'https://ec.toranoana.jp/tora_r/ec/bok/ranking/daily/all'\n",
    "\n",
    "types = [('doujinshi', tora_doujinshi_daily) , ('book', tora_book_daily) ]\n",
    "\n",
    "def extract_daily_rankings(overwrite=False):\n",
    "    \"\"\"Extract daily rankings of doujinshi and books from Toranoana website. Save to disk\"\"\"\n",
    "    data = {}\n",
    "    rankings = {}\n",
    "    for type_, url in types:\n",
    "        soup = send_request(url)\n",
    "        main_div = find_main_product_div(soup)\n",
    "        item_tag_list = get_product_div(main_div)\n",
    "        items = extract_all_info(item_tag_list)\n",
    "        rankings[type_] = items\n",
    "    \n",
    "   \n",
    "    date_groups = extract_date_info(soup)\n",
    "    basename = format_basename(date_groups, format_='d')\n",
    "    data[basename] = rankings\n",
    "    \n",
    "    #save file\n",
    "    filename = format_filename(basename, ext='json')    \n",
    "    #check if file exist, if overwrite is False then stop\n",
    "    data_exist = os.path.isfile(filename) \n",
    "    if data_exist and not overwrite:\n",
    "        print('Daily ranking for ', basename, 'has been retrieved. Exiting...')\n",
    "        return None\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print('Data written to: ', filename)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c3cc6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [('doujinshi', 'https://ec.toranoana.jp/tora_r/ec/cot/ranking/daily/all'),\n",
    "         ('doujinshi', 'https://ec.toranoana.jp/tora_r/ec/cot/ranking/weekly/all'),\n",
    "         ('doujinshi', 'https://ec.toranoana.jp/tora_r/ec/cot/ranking/monthly/all'),\n",
    "         ('book', 'https://ec.toranoana.jp/tora_r/ec/bok/ranking/daily/all'),\n",
    "         ('book', 'https://ec.toranoana.jp/tora_r/ec/bok/ranking/weekly/all'),\n",
    "         ('book', 'https://ec.toranoana.jp/tora_r/ec/bok/ranking/monthly/all')]\n",
    "daily = [tasks[0], tasks[3]]\n",
    "weekly = [tasks[1], tasks[4]]\n",
    "monthly = [tasks[2], tasks[5]]\n",
    "\n",
    "base_url = 'https://ec.toranoana.jp'\n",
    "\n",
    "def extract_data(tasks, duration, overwrite=False):\n",
    "    \"\"\"Extract  rankings of doujinshi and books from Toranoana website. Save to disk\"\"\"\n",
    "    data = {}\n",
    "    rankings = {}\n",
    "    for type_, url in tasks:\n",
    "        print(url)\n",
    "        soup = send_request(url)\n",
    "        main_div = find_main_product_div(soup)\n",
    "        item_div_list = get_product_div(main_div)\n",
    "        items = extract_all_info(item_div_list)\n",
    "#         print(items)\n",
    "        rankings[type_] = items\n",
    "       \n",
    "    date_groups = extract_date_info(soup)\n",
    "    basename = format_basename(date_groups, format_=duration)\n",
    "    data[basename] = rankings\n",
    "    \n",
    "    #save file\n",
    "    filename = format_filename(basename, ext='json')    \n",
    "    #check if file exist, if overwrite is False then stop\n",
    "    data_exist = os.path.isfile(filename) \n",
    "    if data_exist and not overwrite:\n",
    "        print('Data for ', basename, 'already exists. Exiting...')\n",
    "        return None\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print('Data written to: ', filename)\n",
    "    return filename\n",
    "\n",
    "def extract_rankings(duration):\n",
    "    assert duration in ['d', 'w', 'm'] , \"Enter 'd', 'w' or 'm'.\"\n",
    "    if duration == 'd':\n",
    "        tasks = daily\n",
    "    elif duration == 'w':\n",
    "        tasks = weekly\n",
    "    elif duration == 'm':\n",
    "        tasks = monthly\n",
    "    else:\n",
    "        return None\n",
    "    filename = extract_data(tasks, duration, overwrite=False)\n",
    "    return filename\n",
    "\n",
    "def extract_all():\n",
    "    extract_rankings('d')\n",
    "    extract_rankings('w')\n",
    "    extract_rankings('m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e961c649",
   "metadata": {},
   "source": [
    "# -------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef4f3eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ec.toranoana.jp/tora_r/ec/cot/ranking/daily/all\n",
      "200\n",
      "https://ec.toranoana.jp/tora_r/ec/bok/ranking/daily/all\n",
      "200\n",
      "Data written to:  20210614.json\n",
      "https://ec.toranoana.jp/tora_r/ec/cot/ranking/weekly/all\n",
      "200\n",
      "https://ec.toranoana.jp/tora_r/ec/bok/ranking/weekly/all\n",
      "200\n",
      "Data for  20210606_20210612 already exists. Exiting...\n",
      "https://ec.toranoana.jp/tora_r/ec/cot/ranking/monthly/all\n",
      "200\n",
      "https://ec.toranoana.jp/tora_r/ec/bok/ranking/monthly/all\n",
      "200\n",
      "Data for  2021_05 already exists. Exiting...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    extract_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d889b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
